#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
=============================================================================
Master Script - × ×™×ª×•×— ×“×™×•× ×™× ×‘×¤×¨×œ×× ×˜ ×”×‘×¨×™×˜×™
UK Parliament Debates Analysis - Complete Pipeline
=============================================================================

×ª×¨×’×™×œ 1 - ×¢×™×‘×•×“ ×©×¤×” ×˜×‘×¢×™×ª
NLP Assignment 1

×¡×§×¨×™×¤×˜ ×–×” ××‘×¦×¢ ××ª ×›×œ ×©×œ×‘×™ ×”×ª×¨×’×™×œ:
1. ×”×•×¨×“×ª ×§×‘×¦×™ XML
2. × ×™×§×•×™ ×˜×§×¡×˜ ×•×”×¤×¨×“×ª ×¡×™×× ×™ ×¤×™×¡×•×§
3. ×œ××˜×™×–×¦×™×” (Lemmatization)
4. ×‘× ×™×™×ª ××˜×¨×™×¦×•×ª TF-IDF (Word + Lemma)
5. ×‘× ×™×™×ª ××˜×¨×™×¦×•×ª Word2Vec/GloVe (Word + Lemma)
6. ×‘× ×™×™×ª ××˜×¨×™×¦×•×ª SimCSE (Original)
7. ×‘× ×™×™×ª ××˜×¨×™×¦×•×ª SBERT (Original)
8. ×—×™×©×•×‘ ×—×©×™×‘×•×ª ×××¤×™×™× ×™× (Information Gain + Chi-Square)
9. ×™×¦×™×¨×ª ×§×•×‘×¥ Excel ×¢× ×›×œ ×”×ª×•×¦××•×ª

Author: [×©× ×©×œ×š]
Date: November 2025
=============================================================================
"""

import os
import sys
import time
import pickle
import json
import re
import warnings
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple
import logging

# Data processing
import numpy as np
import pandas as pd
from scipy import sparse
from scipy.sparse import csr_matrix, save_npz, load_npz

# Text processing
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import spacy

# ML & NLP
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import mutual_info_classif, chi2
from sklearn.preprocessing import LabelEncoder

# Word embeddings
from gensim.models import Word2Vec
import gensim.downloader as gensim_api

# Transformers
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModel
import torch

# XML parsing
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET

# Excel
import openpyxl
from openpyxl import Workbook
from openpyxl.styles import Font, Alignment, PatternFill

warnings.filterwarnings('ignore')

# ===========================================================================
# ×”×’×“×¨×•×ª ×’×œ×•×‘×œ×™×•×ª / Global Settings
# ===========================================================================

class Config:
    """×”×’×“×¨×•×ª ×”×ª×¦×•×¨×” ×œ×¤×¨×•×™×§×˜"""
    
    # ×ª×™×§×™×•×ª
    RAW_DATA_DIR = "debates_xml"           # ×§×‘×¦×™ XML ××§×•×¨×™×™×
    CLEANED_DIR = "cleaned_texts"          # ×˜×§×¡×˜×™× × ×§×™×™×
    LEMMA_DIR = "lemmatized_texts"         # ×˜×§×¡×˜×™× ××œ×•××˜×™×
    MATRICES_DIR = "matrices"              # ××˜×¨×™×¦×•×ª
    OUTPUT_DIR = "outputs"                 # ×¤×œ×˜×™× ×¡×•×¤×™×™×
    
    # ×¤×¨××˜×¨×™×
    MIN_WORD_FREQ = 5                      # ×ª×“×™×¨×•×ª ××™× ×™××œ×™×ª ×œ××™×œ×”
    MAX_FEATURES = 10000                   # ××§×¡×™××•× ×××¤×™×™× ×™× ×‘-TF-IDF
    EMBEDDING_DIM = 100                    # ×’×•×“×œ ×•×§×˜×•×¨ embedding
    TOP_N_FEATURES = 20                    # ×›××” ×××¤×™×™× ×™× ×œ×”×¦×™×’ ×‘×“×•"×—
    
    # ××•×“×œ×™×
    WORD2VEC_MODEL = "glove-wiki-gigaword-100"  # ××• Word2Vec ×××•××Ÿ
    SBERT_MODEL = "all-MiniLM-L6-v2"
    SIMCSE_MODEL = "princeton-nlp/sup-simcse-bert-base-uncased"
    
    # ×©×¤×”
    LANGUAGE = "english"
    SPACY_MODEL = "en_core_web_sm"

config = Config()

# ===========================================================================
# ×”×’×“×¨×ª Logging
# ===========================================================================

def setup_logging():
    """×”×’×“×¨×ª ××¢×¨×›×ª logging"""
    log_format = '%(asctime)s - %(levelname)s - %(message)s'
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        handlers=[
            logging.FileHandler('parliament_analysis.log', encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return logging.getLogger(__name__)

logger = setup_logging()

# ===========================================================================
# ×©×œ×‘ 1: ×”×•×¨×“×ª ×§×‘×¦×™× (×›×‘×¨ ×™×© ×œ×š ×¡×§×¨×™×¤×˜ × ×¤×¨×“)
# ===========================================================================

def check_downloaded_files():
    """×‘×“×™×§×” ×× ×”×§×‘×¦×™× ×”×•×¨×“×•"""
    if not os.path.exists(config.RAW_DATA_DIR):
        logger.error(f"âŒ ×ª×™×§×™×™×” {config.RAW_DATA_DIR} ×œ× ×§×™×™××ª!")
        logger.info("ğŸ’¡ ×”×¨×¥ ×§×•×“×: python download_debates.py")
        return False
    
    xml_files = list(Path(config.RAW_DATA_DIR).glob("*.xml"))
    logger.info(f"âœ… × ××¦××• {len(xml_files)} ×§×‘×¦×™ XML")
    
    if len(xml_files) < 100:
        logger.warning(f"âš ï¸  × ××¦××• ×¨×§ {len(xml_files)} ×§×‘×¦×™×. ×”×× ×”×”×•×¨×“×” ×”×•×©×œ××”?")
    
    return len(xml_files) > 0

# ===========================================================================
# ×©×œ×‘ 2: × ×™×§×•×™ ×˜×§×¡×˜ ×•×”×¤×¨×“×ª ×¡×™×× ×™ ×¤×™×¡×•×§
# ===========================================================================

class TextCleaner:
    """×× ×§×” ×˜×§×¡×˜ ×•××¤×¨×™×“ ×¡×™×× ×™ ×¤×™×¡×•×§"""
    
    def __init__(self):
        self.punctuation_pattern = re.compile(r'([.,!?;:\'"(){}[\]<>â€¦â€”â€“-])')
        
    def extract_text_from_xml(self, xml_path: str) -> str:
        """×—×™×œ×•×¥ ×˜×§×¡×˜ ××§×•×‘×¥ XML"""
        try:
            with open(xml_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # × ×™×¡×™×•×Ÿ ×œ× ×ª×— ×›-XML
            try:
                soup = BeautifulSoup(content, 'lxml-xml')
                # ×—×™×œ×•×¥ ×›×œ ×”×˜×§×¡×˜
                text = soup.get_text(separator=' ', strip=True)
            except:
                # ×× × ×›×©×œ, ×¤×©×•×˜ ×œ×”×¡×™×¨ ×ª×’×™×•×ª
                text = re.sub(r'<[^>]+>', ' ', content)
            
            # × ×™×§×•×™ ×¨×•×•×—×™× ××™×•×ª×¨×™×
            text = re.sub(r'\s+', ' ', text).strip()
            return text
            
        except Exception as e:
            logger.error(f"×©×’×™××” ×‘×§×¨×™××ª {xml_path}: {e}")
            return ""
    
    def separate_punctuation(self, text: str) -> str:
        """×”×¤×¨×“×ª ×¡×™×× ×™ ×¤×™×¡×•×§ ××”××™×œ×™×"""
        # ×”×•×¡×¤×ª ×¨×•×•×— ×œ×¤× ×™ ×•××—×¨×™ ×›×œ ×¡×™××Ÿ ×¤×™×¡×•×§
        text = self.punctuation_pattern.sub(r' \1 ', text)
        # × ×™×§×•×™ ×¨×•×•×—×™× ××¨×•×‘×™×
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def clean_text(self, text: str) -> str:
        """× ×™×§×•×™ ×›×œ×œ×™ ×©×œ ×”×˜×§×¡×˜"""
        # ×”××¨×” ×œ××•×ª×™×•×ª ×§×˜× ×•×ª
        text = text.lower()
        # ×”×¤×¨×“×ª ×¡×™×× ×™ ×¤×™×¡×•×§
        text = self.separate_punctuation(text)
        return text
    
    def process_file(self, xml_path: str, output_path: str) -> bool:
        """×¢×™×‘×•×“ ×§×•×‘×¥ ×‘×•×“×“"""
        try:
            # ×—×™×œ×•×¥ ×˜×§×¡×˜
            text = self.extract_text_from_xml(xml_path)
            if not text:
                return False
            
            # × ×™×§×•×™
            cleaned = self.clean_text(text)
            
            # ×©××™×¨×”
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(cleaned)
            
            return True
            
        except Exception as e:
            logger.error(f"×©×’×™××” ×‘×¢×™×‘×•×“ {xml_path}: {e}")
            return False
    
    def process_all_files(self):
        """×¢×™×‘×•×“ ×›×œ ×”×§×‘×¦×™×"""
        logger.info("ğŸ§¹ ××ª×—×™×œ × ×™×§×•×™ ×˜×§×¡×˜×™×...")
        
        # ×™×¦×™×¨×ª ×ª×™×§×™×™×ª ×¤×œ×˜
        Path(config.CLEANED_DIR).mkdir(parents=True, exist_ok=True)
        
        # ×§×‘×œ×ª ×¨×©×™××ª ×§×‘×¦×™×
        xml_files = sorted(Path(config.RAW_DATA_DIR).glob("*.xml"))
        
        successful = 0
        failed = 0
        
        for i, xml_path in enumerate(xml_files, 1):
            output_name = xml_path.stem + "_cleaned.txt"
            output_path = Path(config.CLEANED_DIR) / output_name
            
            # ×“×™×œ×•×’ ×¢×œ ×§×‘×¦×™× ×§×™×™××™×
            if output_path.exists():
                successful += 1
                if i % 100 == 0:
                    logger.info(f"  [{i}/{len(xml_files)}] {output_name} - ×§×™×™×, ××“×œ×’")
                continue
            
            if self.process_file(str(xml_path), str(output_path)):
                successful += 1
            else:
                failed += 1
            
            if i % 100 == 0:
                logger.info(f"  [{i}/{len(xml_files)}] ×¢×•×‘×“... (×”×¦×œ×™×—×•: {successful}, × ×›×©×œ×•: {failed})")
        
        logger.info(f"âœ… × ×™×§×•×™ ×”×•×©×œ×: {successful} ×”×¦×œ×™×—×•, {failed} × ×›×©×œ×•")
        return successful > 0

# ===========================================================================
# ×©×œ×‘ 3: ×œ××˜×™×–×¦×™×” (Lemmatization)
# ===========================================================================

class Lemmatizer:
    """××‘×¦×¢ ×œ××˜×™×–×¦×™×” ×¢×œ ×˜×§×¡×˜×™×"""
    
    def __init__(self):
        logger.info("ğŸ“š ×˜×•×¢×Ÿ ××•×“×œ spaCy...")
        try:
            self.nlp = spacy.load(config.SPACY_MODEL)
        except:
            logger.info("××•×¨×™×“ ××•×“×œ spaCy...")
            os.system(f"python -m spacy download {config.SPACY_MODEL}")
            self.nlp = spacy.load(config.SPACY_MODEL)
        
        # ×œ×”×’×‘×™×œ ××ª ×’×•×“×œ ×”×˜×§×¡×˜ ×©spaCy ××¢×‘×“
        self.nlp.max_length = 2000000
    
    def lemmatize_text(self, text: str) -> str:
        """×‘×™×¦×•×¢ ×œ××˜×™×–×¦×™×” ×¢×œ ×˜×§×¡×˜"""
        doc = self.nlp(text)
        lemmas = [token.lemma_ for token in doc]
        return ' '.join(lemmas)
    
    def process_file(self, input_path: str, output_path: str) -> bool:
        """×¢×™×‘×•×“ ×§×•×‘×¥ ×‘×•×“×“"""
        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                text = f.read()
            
            # ×œ××˜×™×–×¦×™×”
            lemmatized = self.lemmatize_text(text)
            
            # ×©××™×¨×”
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(lemmatized)
            
            return True
            
        except Exception as e:
            logger.error(f"×©×’×™××” ×‘×œ××˜×™×–×¦×™×” ×©×œ {input_path}: {e}")
            return False
    
    def process_all_files(self):
        """×¢×™×‘×•×“ ×›×œ ×”×§×‘×¦×™×"""
        logger.info("ğŸ”¤ ××ª×—×™×œ ×œ××˜×™×–×¦×™×”...")
        
        # ×™×¦×™×¨×ª ×ª×™×§×™×™×ª ×¤×œ×˜
        Path(config.LEMMA_DIR).mkdir(parents=True, exist_ok=True)
        
        # ×§×‘×œ×ª ×¨×©×™××ª ×§×‘×¦×™×
        cleaned_files = sorted(Path(config.CLEANED_DIR).glob("*_cleaned.txt"))
        
        successful = 0
        failed = 0
        
        for i, input_path in enumerate(cleaned_files, 1):
            output_name = input_path.stem.replace('_cleaned', '_lemma') + '.txt'
            output_path = Path(config.LEMMA_DIR) / output_name
            
            # ×“×™×œ×•×’ ×¢×œ ×§×‘×¦×™× ×§×™×™××™×
            if output_path.exists():
                successful += 1
                if i % 50 == 0:
                    logger.info(f"  [{i}/{len(cleaned_files)}] {output_name} - ×§×™×™×, ××“×œ×’")
                continue
            
            if self.process_file(str(input_path), str(output_path)):
                successful += 1
            else:
                failed += 1
            
            if i % 50 == 0:
                logger.info(f"  [{i}/{len(cleaned_files)}] ×¢×•×‘×“... (×”×¦×œ×™×—×•: {successful}, × ×›×©×œ×•: {failed})")
        
        logger.info(f"âœ… ×œ××˜×™×–×¦×™×” ×”×•×©×œ××”: {successful} ×”×¦×œ×™×—×•, {failed} × ×›×©×œ×•")
        return successful > 0

# ===========================================================================
# ×©×œ×‘ 4: ×‘× ×™×™×ª ××˜×¨×™×¦×•×ª TF-IDF
# ===========================================================================

class TFIDFBuilder:
    """×‘×•× ×” ××˜×¨×™×¦×•×ª TF-IDF ×¢× BM25"""
    
    def __init__(self):
        self.stop_words = set(stopwords.words(config.LANGUAGE))
    
    def load_documents(self, directory: str) -> Tuple[List[str], List[str]]:
        """×˜×¢×™× ×ª ××¡××›×™× ××ª×™×§×™×™×”"""
        files = sorted(Path(directory).glob("*.txt"))
        documents = []
        filenames = []
        
        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                    if text.strip():
                        documents.append(text)
                        filenames.append(file_path.stem)
            except Exception as e:
                logger.error(f"×©×’×™××” ×‘×§×¨×™××ª {file_path}: {e}")
        
        logger.info(f"  ×˜×¢×•×Ÿ {len(documents)} ××¡××›×™× ×-{directory}")
        return documents, filenames
    
    def build_tfidf_matrix(self, documents: List[str], name: str) -> Tuple[csr_matrix, List[str], TfidfVectorizer]:
        """×‘× ×™×™×ª ××˜×¨×™×¦×ª TF-IDF"""
        logger.info(f"  ×‘×•× ×” ××˜×¨×™×¦×ª TF-IDF: {name}...")
        
        # BM25-like parameters
        vectorizer = TfidfVectorizer(
            max_features=config.MAX_FEATURES,
            min_df=config.MIN_WORD_FREQ,
            stop_words=list(self.stop_words),
            lowercase=True,
            norm='l2',
            use_idf=True,
            smooth_idf=True,
            sublinear_tf=True  # BM25-like
        )
        
        # ×‘× ×™×™×ª ×”××˜×¨×™×¦×”
        tfidf_matrix = vectorizer.fit_transform(documents)
        feature_names = vectorizer.get_feature_names_out()
        
        logger.info(f"    ××˜×¨×™×¦×”: {tfidf_matrix.shape} ({tfidf_matrix.nnz:,} ×¢×¨×›×™× ×©××™× × ××¤×¡)")
        logger.info(f"    ×××¤×™×™× ×™×: {len(feature_names)}")
        
        return tfidf_matrix, feature_names, vectorizer
    
    def save_matrix(self, matrix: csr_matrix, feature_names: List[str], 
                   filenames: List[str], name: str):
        """×©××™×¨×ª ××˜×¨×™×¦×”"""
        output_dir = Path(config.MATRICES_DIR) / name
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ×©××™×¨×ª ×”××˜×¨×™×¦×” (sparse)
        save_npz(output_dir / "matrix.npz", matrix)
        
        # ×©××™×¨×ª ×©××•×ª ×××¤×™×™× ×™×
        with open(output_dir / "feature_names.txt", 'w', encoding='utf-8') as f:
            f.write('\n'.join(feature_names))
        
        # ×©××™×¨×ª ×©××•×ª ×§×‘×¦×™×
        with open(output_dir / "filenames.txt", 'w', encoding='utf-8') as f:
            f.write('\n'.join(filenames))
        
        # ×©××™×¨×ª ××˜×-×“××˜×”
        metadata = {
            'shape': matrix.shape,
            'nnz': matrix.nnz,
            'n_features': len(feature_names),
            'n_documents': len(filenames),
            'created': datetime.now().isoformat()
        }
        with open(output_dir / "metadata.json", 'w') as f:
            json.dump(metadata, f, indent=2)
        
        logger.info(f"    ğŸ’¾ × ×©××¨ ×‘-{output_dir}")
    
    def build_all_matrices(self):
        """×‘× ×™×™×ª ×›×œ ××˜×¨×™×¦×•×ª ×”-TF-IDF"""
        logger.info("ğŸ“Š ×‘×•× ×” ××˜×¨×™×¦×•×ª TF-IDF...")
        
        # TFIDF-Word
        docs_word, files_word = self.load_documents(config.CLEANED_DIR)
        if docs_word:
            matrix_word, features_word, vec_word = self.build_tfidf_matrix(docs_word, "TFIDF-Word")
            self.save_matrix(matrix_word, features_word, files_word, "TFIDF-Word")
        
        # TFIDF-Lemm
        docs_lemma, files_lemma = self.load_documents(config.LEMMA_DIR)
        if docs_lemma:
            matrix_lemma, features_lemma, vec_lemma = self.build_tfidf_matrix(docs_lemma, "TFIDF-Lemm")
            self.save_matrix(matrix_lemma, features_lemma, files_lemma, "TFIDF-Lemm")
        
        logger.info("âœ… ××˜×¨×™×¦×•×ª TF-IDF ×”×•×©×œ××•")
        
        return {
            'word': (matrix_word, features_word, files_word),
            'lemma': (matrix_lemma, features_lemma, files_lemma)
        }

# ===========================================================================
# ×©×œ×‘ 5: ×‘× ×™×™×ª ×•×§×˜×•×¨×™ Word2Vec/GloVe
# ===========================================================================

class Word2VecBuilder:
    """×‘×•× ×” ×•×§×˜×•×¨×™ Word2Vec/GloVe ×œ××¡××›×™×"""
    
    def __init__(self):
        self.stop_words = set(stopwords.words(config.LANGUAGE))
        self.model = None
    
    def load_pretrained_model(self):
        """×˜×¢×™× ×ª ××•×“×œ ×××•××Ÿ ××¨××©"""
        logger.info(f"ğŸ“¥ ×˜×•×¢×Ÿ ××•×“×œ: {config.WORD2VEC_MODEL}...")
        try:
            self.model = gensim_api.load(config.WORD2VEC_MODEL)
            logger.info(f"  âœ… ××•×“×œ × ×˜×¢×Ÿ: {len(self.model)} ××™×œ×™×, ×××“ {self.model.vector_size}")
        except Exception as e:
            logger.error(f"  âŒ ×©×’×™××” ×‘×˜×¢×™× ×ª ××•×“×œ: {e}")
            logger.info("  ğŸ’¡ ××××Ÿ ××•×“×œ ×—×“×©...")
            self.train_word2vec_model()
    
    def train_word2vec_model(self):
        """××™××•×Ÿ ××•×“×œ Word2Vec ×—×“×©"""
        # ×˜×¢×™× ×ª ×›×œ ×”×˜×§×¡×˜×™×
        docs, _ = self.load_documents_for_training(config.CLEANED_DIR)
        
        # ××™××•×Ÿ
        sentences = [doc.split() for doc in docs]
        self.model = Word2Vec(
            sentences=sentences,
            vector_size=config.EMBEDDING_DIM,
            window=5,
            min_count=config.MIN_WORD_FREQ,
            workers=4,
            epochs=10
        )
        
        logger.info(f"  âœ… ××•×“×œ ××•××Ÿ: {len(self.model.wv)} ××™×œ×™×")
    
    def load_documents_for_training(self, directory: str) -> Tuple[List[str], List[str]]:
        """×˜×¢×™× ×ª ××¡××›×™× ××ª×™×§×™×™×” (×œ×œ× ×¤×™×¡×•×§ ×•××¡×¤×¨×™×)"""
        files = sorted(Path(directory).glob("*.txt"))
        documents = []
        filenames = []
        
        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                    # ×”×¡×¨×ª ×¡×™×× ×™ ×¤×™×¡×•×§, ××¡×¤×¨×™×, stopwords
                    text = re.sub(r'[^\w\s]', '', text)  # ×”×¡×¨×ª ×¤×™×¡×•×§
                    text = re.sub(r'\d+', '', text)  # ×”×¡×¨×ª ××¡×¤×¨×™×
                    words = text.split()
                    words = [w for w in words if w not in self.stop_words and len(w) > 2]
                    cleaned_text = ' '.join(words)
                    
                    if cleaned_text.strip():
                        documents.append(cleaned_text)
                        filenames.append(file_path.stem)
            except Exception as e:
                logger.error(f"×©×’×™××” ×‘×§×¨×™××ª {file_path}: {e}")
        
        logger.info(f"  ×˜×¢×•×Ÿ {len(documents)} ××¡××›×™× ×-{directory}")
        return documents, filenames
    
    def document_to_vector(self, document: str) -> np.ndarray:
        """×”××¨×ª ××¡××š ×œ×•×§×˜×•×¨ (×××•×¦×¢ ×©×œ ×•×§×˜×•×¨×™ ×”××™×œ×™×)"""
        words = document.split()
        vectors = []
        
        for word in words:
            if word in self.model.wv:
                vectors.append(self.model.wv[word])
        
        if vectors:
            return np.mean(vectors, axis=0)
        else:
            return np.zeros(self.model.vector_size)
    
    def build_document_vectors(self, documents: List[str], name: str) -> np.ndarray:
        """×‘× ×™×™×ª ××˜×¨×™×¦×ª ×•×§×˜×•×¨×™× ×œ××¡××›×™×"""
        logger.info(f"  ×‘×•× ×” ×•×§×˜×•×¨×™×: {name}...")
        
        vectors = []
        for doc in documents:
            vec = self.document_to_vector(doc)
            vectors.append(vec)
        
        matrix = np.vstack(vectors)
        logger.info(f"    ××˜×¨×™×¦×”: {matrix.shape}")
        
        return matrix
    
    def save_matrix(self, matrix: np.ndarray, filenames: List[str], name: str):
        """×©××™×¨×ª ××˜×¨×™×¦×”"""
        output_dir = Path(config.MATRICES_DIR) / name
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ×©××™×¨×ª ×”××˜×¨×™×¦×”
        np.save(output_dir / "matrix.npy", matrix)
        
        # ×©××™×¨×ª ×©××•×ª ×§×‘×¦×™×
        with open(output_dir / "filenames.txt", 'w', encoding='utf-8') as f:
            f.write('\n'.join(filenames))
        
        # ××˜×-×“××˜×”
        metadata = {
            'shape': matrix.shape,
            'n_documents': len(filenames),
            'embedding_dim': matrix.shape[1],
            'created': datetime.now().isoformat()
        }
        with open(output_dir / "metadata.json", 'w') as f:
            json.dump(metadata, f, indent=2)
        
        logger.info(f"    ğŸ’¾ × ×©××¨ ×‘-{output_dir}")
    
    def build_all_matrices(self):
        """×‘× ×™×™×ª ×›×œ ××˜×¨×™×¦×•×ª Word2Vec/GloVe"""
        logger.info("ğŸ”¤ ×‘×•× ×” ××˜×¨×™×¦×•×ª Word2Vec/GloVe...")
        
        # ×˜×¢×™× ×ª ××•×“×œ
        self.load_pretrained_model()
        
        if self.model is None:
            logger.error("âŒ ×œ× × ×™×ª×Ÿ ×œ×˜×¢×•×Ÿ ××•×“×œ")
            return
        
        # W2V-Word
        docs_word, files_word = self.load_documents_for_training(config.CLEANED_DIR)
        if docs_word:
            matrix_word = self.build_document_vectors(docs_word, "W2V-Word")
            self.save_matrix(matrix_word, files_word, "W2V-Word")
        
        # W2V-Lemm
        docs_lemma, files_lemma = self.load_documents_for_training(config.LEMMA_DIR)
        if docs_lemma:
            matrix_lemma = self.build_document_vectors(docs_lemma, "W2V-Lemm")
            self.save_matrix(matrix_lemma, files_lemma, "W2V-Lemm")
        
        logger.info("âœ… ××˜×¨×™×¦×•×ª Word2Vec/GloVe ×”×•×©×œ××•")

# ===========================================================================
# ×©×œ×‘ 6: SimCSE Embeddings
# ===========================================================================

class SimCSEBuilder:
    """×‘×•× ×” embeddings ×¢× SimCSE"""
    
    def __init__(self):
        logger.info(f"ğŸ¤– ×˜×•×¢×Ÿ ××•×“×œ SimCSE: {config.SIMCSE_MODEL}...")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(config.SIMCSE_MODEL)
            self.model = AutoModel.from_pretrained(config.SIMCSE_MODEL)
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.model.to(self.device)
            logger.info(f"  âœ… ××•×“×œ × ×˜×¢×Ÿ ×¢×œ {self.device}")
        except Exception as e:
            logger.error(f"  âŒ ×©×’×™××” ×‘×˜×¢×™× ×ª ××•×“×œ: {e}")
            self.model = None
    
    def load_original_documents(self) -> Tuple[List[str], List[str]]:
        """×˜×¢×™× ×ª ××¡××›×™× ××§×•×¨×™×™×"""
        xml_files = sorted(Path(config.RAW_DATA_DIR).glob("*.xml"))
        documents = []
        filenames = []
        
        cleaner = TextCleaner()
        
        for xml_path in xml_files:
            text = cleaner.extract_text_from_xml(str(xml_path))
            if text:
                documents.append(text)
                filenames.append(xml_path.stem)
        
        logger.info(f"  ×˜×¢×•×Ÿ {len(documents)} ××¡××›×™× ××§×•×¨×™×™×")
        return documents, filenames
    
    def encode_documents(self, documents: List[str], batch_size: int = 32) -> np.ndarray:
        """×§×™×“×•×“ ××¡××›×™× ×œembeddings"""
        logger.info(f"  ××§×•×“×“ {len(documents)} ××¡××›×™×...")
        
        embeddings = []
        
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i+batch_size]
            
            # Tokenization
            inputs = self.tokenizer(
                batch,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors='pt'
            ).to(self.device)
            
            # Forward pass
            with torch.no_grad():
                outputs = self.model(**inputs)
                # Mean pooling
                batch_embeddings = outputs.last_hidden_state.mean(dim=1)
                embeddings.append(batch_embeddings.cpu().numpy())
            
            if (i // batch_size + 1) % 10 == 0:
                logger.info(f"    ×¢×•×‘×“... {i+len(batch)}/{len(documents)}")
        
        matrix = np.vstack(embeddings)
        logger.info(f"    ××˜×¨×™×¦×”: {matrix.shape}")
        
        return matrix
    
    def build_and_save(self):
        """×‘× ×™×™×ª ×•×©××™×¨×ª embeddings"""
        if self.model is None:
            logger.error("âŒ ××•×“×œ ×œ× ×–××™×Ÿ")
            return
        
        logger.info("ğŸ§  ×‘×•× ×” SimCSE embeddings...")
        
        # ×˜×¢×™× ×ª ××¡××›×™×
        documents, filenames = self.load_original_documents()
        
        if not documents:
            logger.error("âŒ ×œ× × ××¦××• ××¡××›×™×")
            return
        
        # ×§×™×“×•×“
        matrix = self.encode_documents(documents)
        
        # ×©××™×¨×”
        output_dir = Path(config.MATRICES_DIR) / "SimCSE-Origin"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        np.save(output_dir / "matrix.npy", matrix)
        
        with open(output_dir / "filenames.txt", 'w', encoding='utf-8') as f:
            f.write('\n'.join(filenames))
        
        metadata = {
            'shape': matrix.shape,
            'n_documents': len(filenames),
            'embedding_dim': matrix.shape[1],
            'model': config.SIMCSE_MODEL,
            'created': datetime.now().isoformat()
        }
        with open(output_dir / "metadata.json", 'w') as f:
            json.dump(metadata, f, indent=2)
        
        logger.info(f"  ğŸ’¾ × ×©××¨ ×‘-{output_dir}")
        logger.info("âœ… SimCSE embeddings ×”×•×©×œ×")

# ===========================================================================
# ×©×œ×‘ 7: SBERT Embeddings
# ===========================================================================

class SBERTBuilder:
    """×‘×•× ×” embeddings ×¢× Sentence-BERT"""
    
    def __init__(self):
        logger.info(f"ğŸ¤– ×˜×•×¢×Ÿ ××•×“×œ SBERT: {config.SBERT_MODEL}...")
        try:
            self.model = SentenceTransformer(config.SBERT_MODEL)
            logger.info(f"  âœ… ××•×“×œ × ×˜×¢×Ÿ")
        except Exception as e:
            logger.error(f"  âŒ ×©×’×™××” ×‘×˜×¢×™× ×ª ××•×“×œ: {e}")
            self.model = None
    
    def load_original_documents(self) -> Tuple[List[str], List[str]]:
        """×˜×¢×™× ×ª ××¡××›×™× ××§×•×¨×™×™×"""
        xml_files = sorted(Path(config.RAW_DATA_DIR).glob("*.xml"))
        documents = []
        filenames = []
        
        cleaner = TextCleaner()
        
        for xml_path in xml_files:
            text = cleaner.extract_text_from_xml(str(xml_path))
            if text:
                documents.append(text)
                filenames.append(xml_path.stem)
        
        logger.info(f"  ×˜×¢×•×Ÿ {len(documents)} ××¡××›×™× ××§×•×¨×™×™×")
        return documents, filenames
    
    def encode_documents(self, documents: List[str], batch_size: int = 32) -> np.ndarray:
        """×§×™×“×•×“ ××¡××›×™× ×œembeddings"""
        logger.info(f"  ××§×•×“×“ {len(documents)} ××¡××›×™×...")
        
        embeddings = self.model.encode(
            documents,
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        logger.info(f"    ××˜×¨×™×¦×”: {embeddings.shape}")
        return embeddings
    
    def build_and_save(self):
        """×‘× ×™×™×ª ×•×©××™×¨×ª embeddings"""
        if self.model is None:
            logger.error("âŒ ××•×“×œ ×œ× ×–××™×Ÿ")
            return
        
        logger.info("ğŸ§  ×‘×•× ×” SBERT embeddings...")
        
        # ×˜×¢×™× ×ª ××¡××›×™×
        documents, filenames = self.load_original_documents()
        
        if not documents:
            logger.error("âŒ ×œ× × ××¦××• ××¡××›×™×")
            return
        
        # ×§×™×“×•×“
        matrix = self.encode_documents(documents)
        
        # ×©××™×¨×”
        output_dir = Path(config.MATRICES_DIR) / "SBERT-Origin"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        np.save(output_dir / "matrix.npy", matrix)
        
        with open(output_dir / "filenames.txt", 'w', encoding='utf-8') as f:
            f.write('\n'.join(filenames))
        
        metadata = {
            'shape': matrix.shape,
            'n_documents': len(filenames),
            'embedding_dim': matrix.shape[1],
            'model': config.SBERT_MODEL,
            'created': datetime.now().isoformat()
        }
        with open(output_dir / "metadata.json", 'w') as f:
            json.dump(metadata, f, indent=2)
        
        logger.info(f"  ğŸ’¾ × ×©××¨ ×‘-{output_dir}")
        logger.info("âœ… SBERT embeddings ×”×•×©×œ×")

# ===========================================================================
# ×©×œ×‘ 8: ×—×™×©×•×‘ ×—×©×™×‘×•×ª ×××¤×™×™× ×™×
# ===========================================================================

class FeatureImportanceCalculator:
    """××—×©×‘ ×—×©×™×‘×•×ª ×××¤×™×™× ×™×"""
    
    def __init__(self):
        pass
    
    def create_dummy_labels(self, n_samples: int) -> np.ndarray:
        """×™×¦×™×¨×ª ×ª×•×•×™×•×ª ×“××” ×¢×œ ×‘×¡×™×¡ ×–××Ÿ (×—×¦×™ ×¨××©×•×Ÿ ×œ×¢×•××ª ×—×¦×™ ×©× ×™)"""
        # ×¤×™×¦×•×œ ×œ×©×ª×™ ×§×‘×•×¦×•×ª: ×—×¦×™ ×¨××©×•×Ÿ = 0, ×—×¦×™ ×©× ×™ = 1
        labels = np.zeros(n_samples, dtype=int)
        labels[n_samples//2:] = 1
        return labels
    
    def calculate_information_gain(self, X: csr_matrix, y: np.ndarray) -> np.ndarray:
        """×—×™×©×•×‘ Information Gain"""
        logger.info("    ××—×©×‘ Information Gain...")
        scores = mutual_info_classif(X, y, random_state=42)
        return scores
    
    def calculate_chi_square(self, X: csr_matrix, y: np.ndarray) -> np.ndarray:
        """×—×™×©×•×‘ Chi-Square"""
        logger.info("    ××—×©×‘ Chi-Square...")
        # Chi-square ×“×•×¨×© ×¢×¨×›×™× ×œ× ×©×œ×™×œ×™×™×
        X_positive = X.copy()
        X_positive.data = np.abs(X_positive.data)
        scores, _ = chi2(X_positive, y)
        return scores
    
    def load_tfidf_matrix(self, name: str) -> Tuple[csr_matrix, List[str]]:
        """×˜×¢×™× ×ª ××˜×¨×™×¦×ª TF-IDF"""
        matrix_dir = Path(config.MATRICES_DIR) / name
        
        matrix = load_npz(matrix_dir / "matrix.npz")
        
        with open(matrix_dir / "feature_names.txt", 'r', encoding='utf-8') as f:
            feature_names = [line.strip() for line in f]
        
        return matrix, feature_names
    
    def calculate_for_matrix(self, name: str) -> Dict:
        """×—×™×©×•×‘ ×—×©×™×‘×•×ª ×¢×‘×•×¨ ××˜×¨×™×¦×”"""
        logger.info(f"ğŸ“Š ××—×©×‘ ×—×©×™×‘×•×ª ×××¤×™×™× ×™× ×¢×‘×•×¨ {name}...")
        
        # ×˜×¢×™× ×ª ××˜×¨×™×¦×”
        matrix, feature_names = self.load_tfidf_matrix(name)
        
        # ×™×¦×™×¨×ª ×ª×•×•×™×•×ª ×“××”
        labels = self.create_dummy_labels(matrix.shape[0])
        
        # ×—×™×©×•×‘ Information Gain
        ig_scores = self.calculate_information_gain(matrix, labels)
        
        # ×—×™×©×•×‘ Chi-Square
        chi2_scores = self.calculate_chi_square(matrix, labels)
        
        # ×™×¦×™×¨×ª DataFrame
        df = pd.DataFrame({
            'feature': feature_names,
            'information_gain': ig_scores,
            'chi_square': chi2_scores
        })
        
        # ××™×•×Ÿ ×œ×¤×™ Information Gain
        df_ig = df.sort_values('information_gain', ascending=False).reset_index(drop=True)
        
        # ××™×•×Ÿ ×œ×¤×™ Chi-Square
        df_chi = df.sort_values('chi_square', ascending=False).reset_index(drop=True)
        
        logger.info(f"  âœ… ×”×•×©×œ× ×¢×‘×•×¨ {name}")
        
        return {
            'information_gain': df_ig,
            'chi_square': df_chi
        }
    
    def calculate_all(self) -> Dict:
        """×—×™×©×•×‘ ×—×©×™×‘×•×ª ×œ×›×œ ×”××˜×¨×™×¦×•×ª"""
        logger.info("ğŸ¯ ××—×©×‘ ×—×©×™×‘×•×ª ×××¤×™×™× ×™×...")
        
        results = {}
        
        # TFIDF-Word
        try:
            results['TFIDF-Word'] = self.calculate_for_matrix('TFIDF-Word')
        except Exception as e:
            logger.error(f"×©×’×™××” ×‘-TFIDF-Word: {e}")
        
        # TFIDF-Lemm
        try:
            results['TFIDF-Lemm'] = self.calculate_for_matrix('TFIDF-Lemm')
        except Exception as e:
            logger.error(f"×©×’×™××” ×‘-TFIDF-Lemm: {e}")
        
        logger.info("âœ… ×—×™×©×•×‘ ×—×©×™×‘×•×ª ×××¤×™×™× ×™× ×”×•×©×œ×")
        
        return results

# ===========================================================================
# ×©×œ×‘ 9: ×™×¦×™×¨×ª ×§×•×‘×¥ Excel
# ===========================================================================

class ExcelReportGenerator:
    """××™×™×¦×¨ ×“×•"×— Excel ×¢× ×›×œ ×”×ª×•×¦××•×ª"""
    
    def __init__(self, feature_importance_results: Dict):
        self.results = feature_importance_results
        self.workbook = Workbook()
        # ××—×™×§×ª ×’×™×œ×™×•×Ÿ ×‘×¨×™×¨×ª ×”××—×“×œ
        if 'Sheet' in self.workbook.sheetnames:
            del self.workbook['Sheet']
    
    def create_sheet(self, name: str, df: pd.DataFrame, metric: str):
        """×™×¦×™×¨×ª ×’×™×œ×™×•×Ÿ ×‘×•×“×“"""
        ws = self.workbook.create_sheet(title=name[:31])  # Excel ××’×‘×™×œ ×œ-31 ×ª×•×•×™×
        
        # ×›×•×ª×¨×ª
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        header_font = Font(bold=True, color="FFFFFF")
        
        # ×›×ª×™×‘×ª ×›×•×ª×¨×•×ª
        headers = ['Rank', 'Feature', metric.replace('_', ' ').title()]
        for col_idx, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col_idx, value=header)
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = Alignment(horizontal='center')
        
        # ×›×ª×™×‘×ª × ×ª×•× ×™×
        for row_idx, (_, row) in enumerate(df.iterrows(), 2):
            ws.cell(row=row_idx, column=1, value=row_idx-1)  # Rank
            ws.cell(row=row_idx, column=2, value=row['feature'])
            ws.cell(row=row_idx, column=3, value=float(row[metric]))
        
        # ×”×ª×××ª ×¨×•×—×‘ ×¢××•×“×•×ª
        ws.column_dimensions['A'].width = 8
        ws.column_dimensions['B'].width = 30
        ws.column_dimensions['C'].width = 20
    
    def generate(self, output_path: str):
        """×™×¦×™×¨×ª ×”×§×•×‘×¥"""
        logger.info("ğŸ“ ××™×™×¦×¨ ×§×•×‘×¥ Excel...")
        
        for matrix_name, metrics in self.results.items():
            # Information Gain
            df_ig = metrics['information_gain']
            self.create_sheet(f"{matrix_name}-IG", df_ig, 'information_gain')
            
            # Chi-Square
            df_chi = metrics['chi_square']
            self.create_sheet(f"{matrix_name}-Chi2", df_chi, 'chi_square')
        
        # ×©××™×¨×”
        self.workbook.save(output_path)
        logger.info(f"  ğŸ’¾ × ×©××¨ ×‘-{output_path}")
        logger.info("âœ… ×§×•×‘×¥ Excel ×”×•×©×œ×")

# ===========================================================================
# ×©×œ×‘ 10: ××¨×’×•×Ÿ ×•×“×—×™×¡×ª ×§×‘×¦×™×
# ===========================================================================

class FilesOrganizer:
    """×××¨×’×Ÿ ×•×“×•×—×¡ ××ª ×›×œ ×”×§×‘×¦×™× ×œ×”×’×©×”"""
    
    def __init__(self, student_names: str):
        self.student_names = student_names
        self.submission_dir = Path(student_names)
    
    def organize_files(self):
        """××¨×’×•×Ÿ ×”×§×‘×¦×™×"""
        logger.info("ğŸ“¦ ×××¨×’×Ÿ ×§×‘×¦×™× ×œ×”×’×©×”...")
        
        # ×™×¦×™×¨×ª ×ª×™×§×™×™×” ×¨××©×™×ª
        self.submission_dir.mkdir(exist_ok=True)
        
        # ×”×¢×ª×§×ª ×§×•×“
        import shutil
        if os.path.exists('parliament_analysis_master.py'):
            shutil.copy('parliament_analysis_master.py', self.submission_dir / 'code.py')
        
        # ×”×¢×ª×§×ª README (×× ×§×™×™×)
        if os.path.exists('README.pdf'):
            shutil.copy('README.pdf', self.submission_dir)
        elif os.path.exists('README.docx'):
            shutil.copy('README.docx', self.submission_dir)
        
        # ×”×¢×ª×§×ª Excel
        excel_file = Path(config.OUTPUT_DIR) / 'feature_importance_results.xlsx'
        if excel_file.exists():
            shutil.copy(excel_file, self.submission_dir / 'features.xlsx')
        
        logger.info("  âœ… ×§×‘×¦×™× ××•×¨×’× ×•")
    
    def zip_matrices(self):
        """×“×—×™×¡×ª ×›×œ ××˜×¨×™×¦×” ×‘× ×¤×¨×“"""
        logger.info("ğŸ—œï¸  ×“×•×—×¡ ××˜×¨×™×¦×•×ª...")
        
        import zipfile
        
        matrices_dir = Path(config.MATRICES_DIR)
        
        for matrix_folder in matrices_dir.iterdir():
            if matrix_folder.is_dir():
                zip_name = self.submission_dir / f"{matrix_folder.name}.zip"
                
                with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for file in matrix_folder.rglob('*'):
                        if file.is_file():
                            zipf.write(file, file.relative_to(matrices_dir))
                
                logger.info(f"  âœ… {matrix_folder.name}.zip")
        
        logger.info("  âœ… ×›×œ ×”××˜×¨×™×¦×•×ª × ×“×—×¡×•")
    
    def zip_submission(self):
        """×“×—×™×¡×ª ×ª×™×§×™×™×ª ×”×”×’×©×”"""
        logger.info("ğŸ—œï¸  ×“×•×—×¡ ×”×’×©×” ×¡×•×¤×™×ª...")
        
        import zipfile
        
        zip_name = f"{self.student_names}.zip"
        
        with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file in self.submission_dir.rglob('*'):
                if file.is_file():
                    zipf.write(file, file.relative_to(self.submission_dir.parent))
        
        logger.info(f"  ğŸ’¾ × ×•×¦×¨: {zip_name}")
        logger.info("âœ… ×”×’×©×” ××•×›× ×”!")

# ===========================================================================
# ×ª×–××•×Ÿ ×¨××©×™ - Main Pipeline
# ===========================================================================

class MasterPipeline:
    """×× ×”×œ ××ª ×›×œ ×”×ª×”×œ×™×š"""
    
    def __init__(self):
        self.start_time = time.time()
        
        # ×™×¦×™×¨×ª ×ª×™×§×™×•×ª ×‘×¡×™×¡×™×•×ª
        for directory in [config.CLEANED_DIR, config.LEMMA_DIR, 
                         config.MATRICES_DIR, config.OUTPUT_DIR]:
            Path(directory).mkdir(parents=True, exist_ok=True)
    
    def download_nltk_data(self):
        """×”×•×¨×“×ª × ×ª×•× ×™ NLTK"""
        logger.info("ğŸ“š ××•×¨×™×“ × ×ª×•× ×™ NLTK...")
        try:
            nltk.download('stopwords', quiet=True)
            nltk.download('punkt', quiet=True)
            nltk.download('wordnet', quiet=True)
            nltk.download('averaged_perceptron_tagger', quiet=True)
            logger.info("  âœ… × ×ª×•× ×™ NLTK ×”×•×¨×“×•")
        except Exception as e:
            logger.warning(f"  âš ï¸  ×©×’×™××” ×‘×”×•×¨×“×ª NLTK: {e}")
    
    def run_step(self, step_num: int, step_name: str, func, *args, **kwargs):
        """×”×¨×¦×ª ×©×œ×‘ ×‘×•×“×“ ×¢× ××“×™×“×ª ×–××Ÿ"""
        logger.info("")
        logger.info("=" * 70)
        logger.info(f"×©×œ×‘ {step_num}: {step_name}")
        logger.info("=" * 70)
        
        step_start = time.time()
        try:
            result = func(*args, **kwargs)
            step_time = time.time() - step_start
            logger.info(f"â±ï¸  ×–××Ÿ ×©×œ×‘: {step_time/60:.2f} ×“×§×•×ª")
            return result
        except Exception as e:
            logger.error(f"âŒ ×©×’×™××” ×‘×©×œ×‘ {step_num}: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def run_full_pipeline(self, skip_download: bool = True):
        """×”×¨×¦×ª ×›×œ ×”×ª×”×œ×™×š"""
        logger.info("")
        logger.info("*" * 70)
        logger.info("  ğŸš€ ××ª×—×™×œ ×ª×”×œ×™×š × ×™×ª×•×— ×“×™×•× ×™ ×”×¤×¨×œ×× ×˜ ×”×‘×¨×™×˜×™")
        logger.info("*" * 70)
        logger.info("")
        
        # ×”×›× ×•×ª
        self.download_nltk_data()
        
        # ×©×œ×‘ 1: ×‘×“×™×§×ª ×§×‘×¦×™×
        self.run_step(1, "×‘×“×™×§×ª ×§×‘×¦×™× ×©×”×•×¨×“×•", check_downloaded_files)
        
        # ×©×œ×‘ 2: × ×™×§×•×™ ×˜×§×¡×˜
        cleaner = TextCleaner()
        self.run_step(2, "× ×™×§×•×™ ×˜×§×¡×˜ ×•×”×¤×¨×“×ª ×¡×™×× ×™ ×¤×™×¡×•×§", cleaner.process_all_files)
        
        # ×©×œ×‘ 3: ×œ××˜×™×–×¦×™×”
        lemmatizer = Lemmatizer()
        self.run_step(3, "×œ××˜×™×–×¦×™×”", lemmatizer.process_all_files)
        
        # ×©×œ×‘ 4: TF-IDF
        tfidf_builder = TFIDFBuilder()
        self.run_step(4, "×‘× ×™×™×ª ××˜×¨×™×¦×•×ª TF-IDF", tfidf_builder.build_all_matrices)
        
        # ×©×œ×‘ 5: Word2Vec/GloVe
        w2v_builder = Word2VecBuilder()
        self.run_step(5, "×‘× ×™×™×ª ××˜×¨×™×¦×•×ª Word2Vec/GloVe", w2v_builder.build_all_matrices)
        
        # ×©×œ×‘ 6: SimCSE
        simcse_builder = SimCSEBuilder()
        self.run_step(6, "×‘× ×™×™×ª embeddings SimCSE", simcse_builder.build_and_save)
        
        # ×©×œ×‘ 7: SBERT
        sbert_builder = SBERTBuilder()
        self.run_step(7, "×‘× ×™×™×ª embeddings SBERT", sbert_builder.build_and_save)
        
        # ×©×œ×‘ 8: ×—×™×©×•×‘ ×—×©×™×‘×•×ª ×××¤×™×™× ×™×
        importance_calc = FeatureImportanceCalculator()
        results = self.run_step(8, "×—×™×©×•×‘ ×—×©×™×‘×•×ª ×××¤×™×™× ×™×", importance_calc.calculate_all)
        
        # ×©×œ×‘ 9: ×™×¦×™×¨×ª Excel
        if results:
            excel_gen = ExcelReportGenerator(results)
            excel_path = Path(config.OUTPUT_DIR) / 'feature_importance_results.xlsx'
            self.run_step(9, "×™×¦×™×¨×ª ×§×•×‘×¥ Excel", excel_gen.generate, str(excel_path))
        
        # ×¡×™×›×•×
        total_time = time.time() - self.start_time
        logger.info("")
        logger.info("=" * 70)
        logger.info("ğŸ‰ ×”×ª×”×œ×™×š ×”×•×©×œ×!")
        logger.info("=" * 70)
        logger.info(f"â±ï¸  ×–××Ÿ ×›×•×œ×œ: {total_time/60:.2f} ×“×§×•×ª ({total_time/3600:.2f} ×©×¢×•×ª)")
        logger.info("")
        logger.info("ğŸ“ ×§×‘×¦×™× ×©× ×•×¦×¨×•:")
        logger.info(f"  â€¢ ×˜×§×¡×˜×™× × ×§×™×™×: {config.CLEANED_DIR}/")
        logger.info(f"  â€¢ ×˜×§×¡×˜×™× ××œ×•××˜×™×: {config.LEMMA_DIR}/")
        logger.info(f"  â€¢ ××˜×¨×™×¦×•×ª: {config.MATRICES_DIR}/")
        logger.info(f"  â€¢ ×ª×•×¦××•×ª: {config.OUTPUT_DIR}/")
        logger.info("")
        logger.info("ğŸ“‹ ×”×©×œ×‘×™× ×”×‘××™×:")
        logger.info("  1. ×¦×•×¨ ×§×•×‘×¥ README ×¢× ×”×¡×‘×¨×™× ××¤×•×¨×˜×™×")
        logger.info("  2. ×”×¨×¥ organize_submission.py ×œ××¨×’×•×Ÿ ×”×§×‘×¦×™×")
        logger.info("  3. ×”×¢×œ×” ××ª ×”×§×•×‘×¥ ×”××–×•×¤×–×£ ×œ××•×“×œ")
        logger.info("")

# ===========================================================================
# ××¨×’×•×Ÿ ×”×’×©×”
# ===========================================================================

def organize_submission(student_names: str = "StudentNames"):
    """××¨×’×•×Ÿ ×§×‘×¦×™× ×œ×”×’×©×”"""
    organizer = FilesOrganizer(student_names)
    organizer.organize_files()
    organizer.zip_matrices()
    organizer.zip_submission()

# ===========================================================================
# Main Entry Point
# ===========================================================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Master Script - × ×™×ª×•×— ×“×™×•× ×™ ×”×¤×¨×œ×× ×˜ ×”×‘×¨×™×˜×™',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        '--skip-download',
        action='store_true',
        help='×“×œ×’ ×¢×œ ×©×œ×‘ ×”×”×•×¨×“×” (×”×§×‘×¦×™× ×›×‘×¨ ×§×™×™××™×)'
    )
    
    parser.add_argument(
        '--organize',
        action='store_true',
        help='×¨×§ ××¨×’×Ÿ ×§×‘×¦×™× ×œ×”×’×©×”'
    )
    
    parser.add_argument(
        '--student-names',
        type=str,
        default='StudentNames',
        help='×©××•×ª ×”×¡×˜×•×“× ×˜×™× ×œ×”×’×©×”'
    )
    
    args = parser.parse_args()
    
    try:
        if args.organize:
            organize_submission(args.student_names)
        else:
            pipeline = MasterPipeline()
            pipeline.run_full_pipeline(skip_download=args.skip_download)
    
    except KeyboardInterrupt:
        logger.info("\n\nâš ï¸  ×”×ª×”×œ×™×š ×”×•×¤×¡×§ ×¢×œ ×™×“×™ ×”××©×ª××©")
    except Exception as e:
        logger.error(f"\nâŒ ×©×’×™××” ×›×œ×œ×™×ª: {e}")
        import traceback
        traceback.print_exc()